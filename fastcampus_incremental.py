import asyncio
import hashlib
import json
import os
import sys
import time
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright


class IncrementalCrawler:
    """FastCampus ì¦ë¶„ í¬ë¡¤ëŸ¬ - ë³€ê²½ëœ ë°ì´í„°ë§Œ ìˆ˜ì§‘í•˜ëŠ” ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ëŸ¬."""

    # í´ë˜ìŠ¤ ìƒìˆ˜
    MAIN_CATEGORIES = {
        'AI TECH', 'AI CREATIVE', 'AI/ì—…ë¬´ìƒì‚°ì„±', 'ê°œë°œ/ë°ì´í„°', 
        'ë””ìì¸', 'ì˜ìƒ/3D', 'ê¸ˆìœµ/íˆ¬ì', 'ë“œë¡œì‰/ì¼ëŸ¬ìŠ¤íŠ¸', 
        'ë¹„ì¦ˆë‹ˆìŠ¤/ê¸°íš'
    }

    CRAWLING_STEPS = [
        "ì´ˆê¸°í™”",
        "ì´ì „ ë°ì´í„° ë¡œë“œ",
        "ë©”ì¸ ì¹´í…Œê³ ë¦¬ í™•ì¸",
        "í•˜ìœ„ ì¹´í…Œê³ ë¦¬ í™•ì¸", 
        "ê°•ì˜ ëª©ë¡ í™•ì¸",
        "HTML ì €ì¥",
        "ê°•ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘",
        "ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ",
        "ë°ì´í„° ì €ì¥",
        "ì™„ë£Œ"
    ]

    # M3 ì¹© ìµœì í™” ì„¤ì •
    MAX_WORKERS = 4  # ìŠ¤ë ˆë“œ í’€ ì›Œì»¤ ìˆ˜
    MAX_CONCURRENT_COURSES = 8  # ë™ì‹œ ê°•ì˜ ìˆ˜ì§‘ ìˆ˜

    TIMEOUTS = {
        'default': 30000,
        'navigation': 60000,
        'image_load': 10000
    }

    HEADERS = {
        'default': {
            'User-Agent': ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                          'AppleWebKit/537.36'),
            'Accept': ('text/html,application/xhtml+xml,'
                      'application/xml;q=0.9,image/webp,*/*;q=0.8'),
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive'
        },
        'image': {
            'User-Agent': ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                          'AppleWebKit/537.36'),
            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
        }
    }

    SELECTORS = {
        'main_category': ('div.GNBDesktopCategoryItem_container__ln5E6 '
                         'a[href*="category_online"]'),
        'sub_category': ('li.GNBDesktopCategoryItem_subCategory__twmcG '
                        'a[href*="category_online"]'),
        'course_card': '[data-e2e="course-card"], .course-card, .course-item',
        'course_link': 'a[href*="/data_online_"]',
        'category_nav': '[data-e2e="navigation-category"]'
    }

    # ì´ë¯¸ì§€ ê´€ë ¨ ìƒìˆ˜
    VALID_IMAGE_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.webp', '.gif', '.svg']
    MIN_IMAGE_SIZE = 1000
    MAX_THUMBNAILS = 2

    def __init__(self, base_url="https://fastcampus.co.kr/", 
                 output_dir="./incremental_crawl"):
        """ì¦ë¶„ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”."""
        self.base_url = base_url
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # í˜„ì¬ í¬ë¡¤ë§ ë°ì´í„°
        self.current_data = {
            'main_categories': [],
            'sub_categories': [],
            'courses': [],
            'course_details': []
        }

        # ì´ì „ í¬ë¡¤ë§ ë°ì´í„°
        self.previous_data = {
            'main_categories': [],
            'sub_categories': [],
            'courses': [],
            'course_details': []
        }

        # ì¦ë¶„ í¬ë¡¤ë§ í†µê³„
        self.incremental_stats = {
            'main_categories': {'new': 0, 'updated': 0, 'removed': 0, 'unchanged': 0},
            'sub_categories': {'new': 0, 'updated': 0, 'removed': 0, 'unchanged': 0},
            'courses': {'new': 0, 'updated': 0, 'removed': 0, 'unchanged': 0},
            'course_details': {'new': 0, 'updated': 0, 'removed': 0, 'unchanged': 0}
        }

        self.seen_urls = set()
        self.current_step = 0
        self.start_time = None
        self.course_titles = {}
        self.is_first_run = False

        # ì§„í–‰ë¥  ì¶”ì 
        self.progress = {
            'total_subcategories': 0,
            'completed_subcategories': 0,
            'total_courses': 0,
            'completed_courses': 0,
            'total_details': 0,
            'completed_details': 0
        }

        self._setup_directories()
        self._log_progress("ì¦ë¶„ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")

    def _setup_directories(self):
        """ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±."""
        directories = [
            "main_categories", "sub_categories", "courses",
            "sumnail_images", "lect_images", "json_files", "incremental_logs"
        ]

        for directory in directories:
            (self.output_dir / directory).mkdir(exist_ok=True)

    def _log_progress(self, message, step=None):
        """ì§„í–‰ ìƒí™© ë¡œê¹…."""
        if step is not None:
            self.current_step = step

        timestamp = datetime.now().strftime("%H:%M:%S")
        step_name = (self.CRAWLING_STEPS[self.current_step] 
                    if self.current_step < len(self.CRAWLING_STEPS) 
                    else "ì•Œ ìˆ˜ ì—†ìŒ")

        print(f"[{timestamp}] [{step_name}] {message}")
        sys.stdout.flush()

    def _log_step_start(self, step_name):
        """ë‹¨ê³„ ì‹œì‘ ë¡œê¹…."""
        self.current_step = self.CRAWLING_STEPS.index(step_name)
        self._log_progress(f"{step_name} ì‹œì‘")

    def _log_step_complete(self, step_name, count=None):
        """ë‹¨ê³„ ì™„ë£Œ ë¡œê¹…."""
        if count is not None:
            self._log_progress(f"{step_name} ì™„ë£Œ - {count}ê°œ í•­ëª© ìˆ˜ì§‘")
        else:
            self._log_progress(f"{step_name} ì™„ë£Œ")

    def _log_error(self, message, error=None):
        """ì—ëŸ¬ ë¡œê¹…."""
        timestamp = datetime.now().strftime("%H:%M:%S")
        if error:
            print(f"[{timestamp}] [ERROR] {message}: {str(error)}")
        else:
            print(f"[{timestamp}] [ERROR] {message}")
        sys.stdout.flush()

    def _log_incremental_stats(self):
        """ì¦ë¶„ í¬ë¡¤ë§ í†µê³„ ë¡œê¹…."""
        self._log_progress("=== ì¦ë¶„ í¬ë¡¤ë§ í†µê³„ ===")
        
        for data_type, stats in self.incremental_stats.items():
            total = sum(stats.values())
            if total > 0:
                self._log_progress(f"{data_type}: ì‹ ê·œ {stats['new']}ê°œ, "
                                 f"ì—…ë°ì´íŠ¸ {stats['updated']}ê°œ, "
                                 f"ì‚­ì œ {stats['removed']}ê°œ, "
                                 f"ë³€ê²½ì—†ìŒ {stats['unchanged']}ê°œ")

    def _generate_content_hash(self, content):
        """ì½˜í…ì¸ ì˜ í•´ì‹œê°’ ìƒì„±."""
        if isinstance(content, dict):
            content = json.dumps(content, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(str(content).encode()).hexdigest()

    def _load_previous_data(self):
        """ì´ì „ í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ."""
        self._log_step_start("ì´ì „ ë°ì´í„° ë¡œë“œ")
        
        data_files = {
            'main_categories': 'main_categories.json',
            'sub_categories': 'sub_categories.json',
            'courses': 'courses_list.json',
            'course_details': 'course_details.json'
        }

        for data_type, filename in data_files.items():
            file_path = self.output_dir / "json_files" / filename
            
            if file_path.exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        self.previous_data[data_type] = json.load(f)
                    self._log_progress(f"{data_type} ì´ì „ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: "
                                     f"{len(self.previous_data[data_type])}ê°œ")
                except Exception as e:
                    self._log_error(f"{data_type} ì´ì „ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨", e)
                    self.previous_data[data_type] = []
            else:
                self._log_progress(f"{data_type} ì´ì „ ë°ì´í„° ì—†ìŒ (ì²« ì‹¤í–‰)")
                self.previous_data[data_type] = []
                self.is_first_run = True

        self._log_step_complete("ì´ì „ ë°ì´í„° ë¡œë“œ")

    def _compare_data_changes(self, data_type, current_data, previous_data):
        """ë°ì´í„° ë³€ê²½ì‚¬í•­ ë¹„êµ."""
        current_urls = set()
        previous_urls = set()
        
        # URL ê¸°ë°˜ ë¹„êµë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        if data_type in ['main_categories', 'sub_categories']:
            current_urls = set(item.get('ë©”ì¸ì¹´í…Œê³ ë¦¬ë§í¬', item.get('í•˜ìœ„ì¹´í…Œê³ ë¦¬ë§í¬', '')) 
                             for item in current_data)
            previous_urls = set(item.get('ë©”ì¸ì¹´í…Œê³ ë¦¬ë§í¬', item.get('í•˜ìœ„ì¹´í…Œê³ ë¦¬ë§í¬', '')) 
                              for item in previous_data)
        elif data_type == 'courses':
            current_urls = set(item.get('ê°•ì˜ë§í¬', '') for item in current_data)
            previous_urls = set(item.get('ê°•ì˜ë§í¬', '') for item in previous_data)
        elif data_type == 'course_details':
            current_urls = set(item.get('ê°•ì˜ë§í¬', '') for item in current_data)
            previous_urls = set(item.get('ê°•ì˜ë§í¬', '') for item in previous_data)

        # ë³€ê²½ì‚¬í•­ ë¶„ë¥˜
        new_items = current_urls - previous_urls
        removed_items = previous_urls - current_urls
        unchanged_items = current_urls & previous_urls

        # ì—…ë°ì´íŠ¸ëœ í•­ëª© ì°¾ê¸° (URLì€ ê°™ì§€ë§Œ ë‚´ìš©ì´ ë‹¤ë¥¸ ê²½ìš°)
        updated_items = set()
        if data_type in ['courses', 'course_details']:
            for item in current_data:
                url = item.get('ê°•ì˜ë§í¬', '')
                if url in unchanged_items:
                    # ì´ì „ ë°ì´í„°ì—ì„œ ê°™ì€ URLì˜ í•­ëª© ì°¾ê¸°
                    previous_item = next((p for p in previous_data 
                                        if p.get('ê°•ì˜ë§í¬', '') == url), None)
                    if previous_item:
                        current_hash = self._generate_content_hash(item)
                        previous_hash = self._generate_content_hash(previous_item)
                        if current_hash != previous_hash:
                            updated_items.add(url)

        # í†µê³„ ì—…ë°ì´íŠ¸
        self.incremental_stats[data_type] = {
            'new': len(new_items),
            'updated': len(updated_items),
            'removed': len(removed_items),
            'unchanged': len(unchanged_items) - len(updated_items)
        }

        return {
            'new': new_items,
            'updated': updated_items,
            'removed': removed_items,
            'unchanged': unchanged_items - updated_items
        }

    def _setup_browser(self, playwright):
        """ë¸Œë¼ìš°ì € ì„¤ì •."""
        browser = playwright.chromium.launch(headless=True)
        context = browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent=self.HEADERS['default']['User-Agent'],
            extra_http_headers=self.HEADERS['default']
        )
        page = context.new_page()
        page.set_default_timeout(self.TIMEOUTS['default'])
        page.set_default_navigation_timeout(self.TIMEOUTS['navigation'])
        return browser, page

    def _safe_page_load(self, page, url, retries=3):
        """ì•ˆì „í•œ í˜ì´ì§€ ë¡œë”©."""
        for attempt in range(retries):
            try:
                page.goto(url, timeout=self.TIMEOUTS['navigation'])
                page.wait_for_load_state("networkidle", timeout=30000)
                return True
            except Exception:
                if attempt < retries - 1:
                    page.wait_for_timeout(3000)
        return False

    def _scroll_page(self, page):
        """ë™ì  ì½˜í…ì¸  ë¡œë“œë¥¼ ìœ„í•œ ìŠ¤í¬ë¡¤."""
        last_height = page.evaluate("document.body.scrollHeight")

        while True:
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            page.wait_for_timeout(2000)
            new_height = page.evaluate("document.body.scrollHeight")

            if new_height == last_height:
                break
            last_height = new_height

    def _wait_for_images_to_load(self, page, timeout=30000):
        """ì´ë¯¸ì§€ ì™„ì „ ë¡œë”© ëŒ€ê¸°."""
        try:
            # ëª¨ë“  ì´ë¯¸ì§€ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            page.wait_for_function("""
                () => {
                    const images = Array.from(document.querySelectorAll('img'));
                    return images.every(img => {
                        return img.complete && img.naturalHeight !== 0;
                    });
                }
            """, timeout=timeout)

            # lazy loading ì´ë¯¸ì§€ë“¤ ê°•ì œ ë¡œë“œ
            page.evaluate("""
                () => {
                    const lazyImages = document.querySelectorAll('img[data-src]');
                    lazyImages.forEach(img => {
                        if (img.dataset.src) {
                            img.src = img.dataset.src;
                        }
                    });
                }
            """)

            # srcset ì†ì„± ì •ê·œí™”
            page.evaluate("""
                () => {
                    const sources = document.querySelectorAll('source[srcset]');
                    sources.forEach(source => {
                        if (source.srcset && source.srcset.trim()) {
                            const baseUrl = window.location.origin;
                            source.srcset = source.srcset.split(',').map(src => {
                                src = src.trim().split(' ')[0];
                                if (src.startsWith('/')) {
                                    return baseUrl + src;
                                } else if (src.startsWith('//')) {
                                    return 'https:' + src;
                                }
                                return src;
                            }).join(', ');
                        }
                    });
                }
            """)

            self._log_progress("ì´ë¯¸ì§€ ë¡œë”© ì™„ë£Œ")
            return True

        except Exception as e:
            self._log_error(f"ì´ë¯¸ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜", e)
            return False

    def _prepare_navigation(self, page):
        """ë„¤ë¹„ê²Œì´ì…˜ ì¤€ë¹„."""
        try:
            # íŒì—… ë§ˆìŠ¤í¬ ì œê±°
            popup_mask = page.locator('.fc-popup-mask')
            if popup_mask.is_visible():
                popup_mask.evaluate('element => element.remove()')
                page.wait_for_timeout(1000)
        except Exception:
            pass

        try:
            # í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            page.wait_for_load_state("networkidle", timeout=30000)

            # ì¹´í…Œê³ ë¦¬ ë²„íŠ¼ í´ë¦­ - ì—¬ëŸ¬ ì„ íƒì ì‹œë„
            selectors = [
                '[data-e2e="navigation-category"]',
                '.category-button',
                '.nav-category', 
                '.gnb-category',
                'button[aria-label*="ì¹´í…Œê³ ë¦¬"]',
                'button:has-text("ì¹´í…Œê³ ë¦¬")'
            ]

            clicked = False
            for selector in selectors:
                try:
                    button = page.locator(selector).first
                    if button.is_visible(timeout=5000):
                        button.click(timeout=10000)
                        page.wait_for_timeout(3000)
                        clicked = True
                        self._log_progress(f"ì¹´í…Œê³ ë¦¬ ë²„íŠ¼ í´ë¦­ ì„±ê³µ: {selector}")
                        break
                except Exception:
                    continue

            if not clicked:
                self._log_error("ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë²„íŠ¼ ì„ íƒì ì‹¤íŒ¨")

        except Exception as e:
            self._log_error(f"ë„¤ë¹„ê²Œì´ì…˜ ì¤€ë¹„ ì‹¤íŒ¨", e)

    def _extract_main_categories(self, page):
        """ë©”ì¸ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ."""
        self._log_step_start("ë©”ì¸ ì¹´í…Œê³ ë¦¬ í™•ì¸")
        self._prepare_navigation(page)
        links = page.locator(self.SELECTORS['main_category']).all()
        seen = set()

        for i, link in enumerate(links, 1):
            try:
                name = link.text_content().strip()
                url = link.get_attribute('href')

                if (name and url and name in self.MAIN_CATEGORIES 
                    and name not in seen):
                    seen.add(name)

                    if url.startswith('/'):
                        url = urljoin(self.base_url, url)

                    self.current_data['main_categories'].append({
                        "ë©”ì¸ì¹´í…Œê³ ë¦¬": name,
                        "ë©”ì¸ì¹´í…Œê³ ë¦¬ë§í¬": url,
                        "ìˆ˜ì§‘ì¼ì‹œ": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    })
            except Exception as e:
                self._log_error(f"ë©”ì¸ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë§í¬ {i})", e)
                continue

        # ë³€ê²½ì‚¬í•­ ë¹„êµ
        changes = self._compare_data_changes('main_categories', 
                                           self.current_data['main_categories'],
                                           self.previous_data['main_categories'])

        if changes['new'] or changes['updated'] or changes['removed']:
            self._log_progress(f"ë©”ì¸ ì¹´í…Œê³ ë¦¬ ë³€ê²½ì‚¬í•­ ë°œê²¬: "
                             f"ì‹ ê·œ {len(changes['new'])}ê°œ, "
                             f"ì—…ë°ì´íŠ¸ {len(changes['updated'])}ê°œ, "
                             f"ì‚­ì œ {len(changes['removed'])}ê°œ")
        else:
            self._log_progress("ë©”ì¸ ì¹´í…Œê³ ë¦¬ ë³€ê²½ì‚¬í•­ ì—†ìŒ")

        self._log_step_complete("ë©”ì¸ ì¹´í…Œê³ ë¦¬ í™•ì¸", 
                               len(self.current_data['main_categories']))

    def _find_parent_category(self, sub_link):
        """ìƒìœ„ ì¹´í…Œê³ ë¦¬ ì°¾ê¸°."""
        try:
            xpath = ('xpath=ancestor::div[contains(@class, '
                    '"GNBDesktopCategoryItem_container__ln5E6")]')
            parent = sub_link.locator(xpath).first

            if parent.count() > 0:
                main_link = parent.locator('a[href*="category_online"]').first
                if main_link.count() > 0:
                    return main_link.text_content().strip()
        except Exception:
            pass
        return None

    def _extract_sub_categories(self, page):
        """í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ."""
        self._log_step_start("í•˜ìœ„ ì¹´í…Œê³ ë¦¬ í™•ì¸")
        self._prepare_navigation(page)
        links = page.locator(self.SELECTORS['sub_category']).all()

        for i, link in enumerate(links, 1):
            try:
                name = link.text_content().strip()
                url = link.get_attribute('href')

                if not name or not url or url in self.seen_urls:
                    continue

                self.seen_urls.add(url)
                parent = self._find_parent_category(link)

                if parent:
                    if url.startswith('/'):
                        url = urljoin(self.base_url, url)

                    self.current_data['sub_categories'].append({
                        "ë©”ì¸ì¹´í…Œê³ ë¦¬": parent,
                        "í•˜ìœ„ì¹´í…Œê³ ë¦¬": name,
                        "í•˜ìœ„ì¹´í…Œê³ ë¦¬ë§í¬": url,
                        "ìˆ˜ì§‘ì¼ì‹œ": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    })
            except Exception as e:
                self._log_error(f"í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë§í¬ {i})", e)
                continue

        # ë³€ê²½ì‚¬í•­ ë¹„êµ
        changes = self._compare_data_changes('sub_categories',
                                           self.current_data['sub_categories'],
                                           self.previous_data['sub_categories'])

        if changes['new']:
            self._log_progress(f"ì‹ ê·œ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ {len(changes['new'])}ê°œ ë°œê²¬")
            for new_url in changes['new']:
                new_category = next((cat for cat in self.current_data['sub_categories']
                                   if cat.get('í•˜ìœ„ì¹´í…Œê³ ë¦¬ë§í¬') == new_url), None)
                if new_category:
                    self._log_progress(f"  - {new_category['ë©”ì¸ì¹´í…Œê³ ë¦¬']} > "
                                     f"{new_category['í•˜ìœ„ì¹´í…Œê³ ë¦¬']}")

        self.progress['total_subcategories'] = len(self.current_data['sub_categories'])
        self._log_step_complete("í•˜ìœ„ ì¹´í…Œê³ ë¦¬ í™•ì¸", 
                               len(self.current_data['sub_categories']))

    def _collect_courses_incremental(self):
        """ì¦ë¶„ ê°•ì˜ ìˆ˜ì§‘."""
        self._log_step_start("ê°•ì˜ ëª©ë¡ í™•ì¸")
        
        # ì‹ ê·œ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ì—ì„œ ê°•ì˜ ìˆ˜ì§‘
        new_subcategories = []
        for category in self.current_data['sub_categories']:
            url = category['í•˜ìœ„ì¹´í…Œê³ ë¦¬ë§í¬']
            if not any(p.get('í•˜ìœ„ì¹´í…Œê³ ë¦¬ë§í¬') == url 
                      for p in self.previous_data['sub_categories']):
                new_subcategories.append(category)

        if new_subcategories:
            self._log_progress(f"ì‹ ê·œ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ {len(new_subcategories)}ê°œì—ì„œ "
                             f"ê°•ì˜ ìˆ˜ì§‘ ì‹œì‘")
            
            with ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:
                futures = []
                for subcategory in new_subcategories:
                    future = executor.submit(
                        self._extract_courses_from_subcategory, 
                        subcategory, 
                        max_courses=20
                    )
                    futures.append(future)

                for i, future in enumerate(futures):
                    try:
                        courses = future.result(timeout=300)
                        if courses:
                            self.current_data['courses'].extend(courses)
                            self._log_progress(f"ì‹ ê·œ ì¹´í…Œê³ ë¦¬ ê°•ì˜ ìˆ˜ì§‘ ì™„ë£Œ: "
                                             f"{len(courses)}ê°œ")
                    except Exception as e:
                        self._log_error(f"ì‹ ê·œ ì¹´í…Œê³ ë¦¬ ê°•ì˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {i+1}", e)

        # ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ì—ì„œ ê°•ì˜ ëª©ë¡ë§Œ ë¹ ë¥´ê²Œ í™•ì¸
        self._check_existing_courses()

        # ë³€ê²½ì‚¬í•­ ë¹„êµ
        changes = self._compare_data_changes('courses',
                                           self.current_data['courses'],
                                           self.previous_data['courses'])

        if changes['new']:
            self._log_progress(f"ì‹ ê·œ ê°•ì˜ {len(changes['new'])}ê°œ ë°œê²¬")
        if changes['updated']:
            self._log_progress(f"ì—…ë°ì´íŠ¸ëœ ê°•ì˜ {len(changes['updated'])}ê°œ ë°œê²¬")

        self._log_step_complete("ê°•ì˜ ëª©ë¡ í™•ì¸", len(self.current_data['courses']))

    def _check_existing_courses(self):
        """ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ì—ì„œ ê°•ì˜ ëª©ë¡ ë¹ ë¥¸ í™•ì¸."""
        # ê¸°ì¡´ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ì—ì„œ ê°•ì˜ ëª©ë¡ë§Œ ë¹ ë¥´ê²Œ í™•ì¸
        existing_categories = []
        for category in self.current_data['sub_categories']:
            url = category['í•˜ìœ„ì¹´í…Œê³ ë¦¬ë§í¬']
            if any(p.get('í•˜ìœ„ì¹´í…Œê³ ë¦¬ë§í¬') == url 
                  for p in self.previous_data['sub_categories']):
                existing_categories.append(category)

        if existing_categories:
            self._log_progress(f"ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ {len(existing_categories)}ê°œì—ì„œ "
                             f"ê°•ì˜ ëª©ë¡ ë¹ ë¥¸ í™•ì¸")
            
            # ìƒ˜í”Œë§ìœ¼ë¡œ ì¼ë¶€ ì¹´í…Œê³ ë¦¬ë§Œ í™•ì¸ (ì„±ëŠ¥ ìµœì í™”)
            sample_size = min(5, len(existing_categories))
            sample_categories = existing_categories[:sample_size]
            
            for category in sample_categories:
                try:
                    with sync_playwright() as p:
                        browser, page = self._setup_browser(p)
                        
                        if self._safe_page_load(page, category['í•˜ìœ„ì¹´í…Œê³ ë¦¬ë§í¬']):
                            self._scroll_page(page)
                            page.wait_for_timeout(2000)
                            
                            # ê°•ì˜ ì¹´ë“œ ê°œìˆ˜ë§Œ ë¹ ë¥´ê²Œ í™•ì¸
                            cards = page.locator(self.SELECTORS['course_card']).all()
                            self._log_progress(f"ì¹´í…Œê³ ë¦¬ '{category['í•˜ìœ„ì¹´í…Œê³ ë¦¬']}': "
                                             f"{len(cards)}ê°œ ê°•ì˜ í™•ì¸")
                        
                        browser.close()
                except Exception as e:
                    self._log_error(f"ì¹´í…Œê³ ë¦¬ í™•ì¸ ì‹¤íŒ¨: {category['í•˜ìœ„ì¹´í…Œê³ ë¦¬']}", e)

    def _extract_courses_from_subcategory(self, subcategory, max_courses=20):
        """í•˜ìœ„ ì¹´í…Œê³ ë¦¬ì—ì„œ ê°•ì˜ ì •ë³´ ì¶”ì¶œ."""
        with sync_playwright() as p:
            browser, page = self._setup_browser(p)

            try:
                if not self._safe_page_load(page, subcategory['í•˜ìœ„ì¹´í…Œê³ ë¦¬ë§í¬']):
                    return []

                self._scroll_page(page)
                page.wait_for_timeout(3000)

                # ê°•ì˜ ì¹´ë“œ ì°¾ê¸°
                card_selectors = [
                    '[data-e2e="course-card"]',
                    '.course-card', 
                    '.course-item',
                    'div[class*="CourseCard"]'
                ]

                cards = []
                for selector in card_selectors:
                    found_cards = page.locator(selector).all()
                    if found_cards:
                        cards = found_cards
                        break

                if not cards:
                    cards = page.locator(self.SELECTORS['course_link']).all()

                collected_courses = []
                for i, card in enumerate(cards[:max_courses], 1):
                    try:
                        url = self._extract_course_url(card)
                        if not url:
                            continue

                        if url.startswith('/'):
                            url = urljoin(self.base_url, url)

                        if '/event_online_' in url or url in self.seen_urls:
                            continue

                        self.seen_urls.add(url)
                        title = self._extract_title(card, url)

                        course_data = {
                            "ë©”ì¸ì¹´í…Œê³ ë¦¬": subcategory['ë©”ì¸ì¹´í…Œê³ ë¦¬'],
                            "í•˜ìœ„ì¹´í…Œê³ ë¦¬": subcategory['í•˜ìœ„ì¹´í…Œê³ ë¦¬'],
                            "ê°•ì˜ì œëª©": title,
                            "ê°•ì˜ë§í¬": url,
                            "ìˆ˜ì§‘ì¼ì‹œ": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }

                        collected_courses.append(course_data)
                    except Exception as e:
                        continue

                return collected_courses

            finally:
                browser.close()

    def _extract_course_url(self, card):
        """ê°•ì˜ URL ì¶”ì¶œ."""
        try:
            url = card.get_attribute('href')
            if url:
                return url
        except Exception:
            pass

        try:
            link = card.locator(self.SELECTORS['course_link']).first
            if link.count() > 0:
                return link.get_attribute('href')
        except Exception:
            pass

        return None

    def _extract_title(self, card, course_url=None):
        """ê°•ì˜ ì œëª© ì¶”ì¶œ."""
        selectors = [
            '.CourseCard_courseCardTitle__1HQgO',
            '[data-e2e="display-card"]',
            'h3', 'h4', '.course-title', '.title', 
            '[data-e2e="course-title"]', '.course-name'
        ]

        for selector in selectors:
            element = card.locator(selector).first
            if element.count() > 0:
                title = element.text_content().strip()
                if (title and len(title) > 3 and not title.isdigit() 
                    and not title.endswith('+')):
                    return title

        return "ì œëª© ì—†ìŒ"

    def _collect_course_details_incremental(self):
        """ì¦ë¶„ ê°•ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘."""
        self._log_step_start("ê°•ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘")
        
        # ì‹ ê·œ/ì—…ë°ì´íŠ¸ëœ ê°•ì˜ë§Œ ìƒì„¸ ìˆ˜ì§‘
        target_courses = []
        changes = self._compare_data_changes('courses',
                                           self.current_data['courses'],
                                           self.previous_data['courses'])
        
        # ì‹ ê·œ ê°•ì˜
        for course in self.current_data['courses']:
            if course['ê°•ì˜ë§í¬'] in changes['new']:
                target_courses.append(course)
        
        # ì—…ë°ì´íŠ¸ëœ ê°•ì˜
        for course in self.current_data['courses']:
            if course['ê°•ì˜ë§í¬'] in changes['updated']:
                target_courses.append(course)

        if not target_courses:
            self._log_progress("ì²˜ë¦¬í•  ê°•ì˜ ìƒì„¸ ì •ë³´ ì—†ìŒ")
            self._log_step_complete("ê°•ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘")
            return

        self._log_progress(f"ì‹ ê·œ/ì—…ë°ì´íŠ¸ ê°•ì˜ {len(target_courses)}ê°œ ìƒì„¸ ìˆ˜ì§‘ ì‹œì‘")

        with ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:
            futures = []
            for course in target_courses:
                future = executor.submit(
                    self._extract_course_detail, 
                    course['ê°•ì˜ë§í¬']
                )
                futures.append(future)

            for i, future in enumerate(futures):
                try:
                    detail = future.result(timeout=300)
                    if detail:
                        self.current_data['course_details'].append(detail)
                        if (i + 1) % 5 == 0:
                            self._log_progress(f"ê°•ì˜ ìƒì„¸ ì™„ë£Œ: {i+1}/{len(futures)}")
                except Exception as e:
                    self._log_error(f"ê°•ì˜ ìƒì„¸ ì²˜ë¦¬ ì‹¤íŒ¨: {i+1}", e)

        self._log_step_complete("ê°•ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘", 
                              len(self.current_data['course_details']))

    def _extract_course_detail(self, course_url):
        """ê°•ì˜ ìƒì„¸ ì •ë³´ ì¶”ì¶œ."""
        with sync_playwright() as p:
            browser, page = self._setup_browser(p)

            try:
                course_name = course_url.split('/')[-1]

                if not self._safe_page_load(page, course_url):
                    return None

                self._scroll_page(page)
                page.wait_for_timeout(3000)

                # HTML ì €ì¥
                html_content = page.content()
                safe_name = course_url.split('/')[-1]

                with open(self.output_dir / f"courses/{safe_name}.html", 
                          'w', encoding='utf-8') as f:
                    f.write(html_content)

                soup = BeautifulSoup(html_content, 'html.parser')

                # ì œëª© ì¶”ì¶œ
                title_selectors = ['h1', '[data-e2e="course-title"]', '.title']
                title = "ì œëª© ì—†ìŒ"

                for selector in title_selectors:
                    element = soup.select_one(selector)
                    if element and element.get_text().strip():
                        title = element.get_text().strip()
                        if title != "root layout":
                            break

                return {
                    "ê°•ì˜ëª…": title,
                    "ê°•ì˜ë§í¬": course_url,
                    "ìˆ˜ì§‘ì¼ì‹œ": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "ì¶”ì¶œëœ_í…ìŠ¤íŠ¸": self._extract_page_content(soup)
                }

            finally:
                browser.close()

    def _extract_page_content(self, soup):
        """í˜ì´ì§€ í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì¶”ì¶œ."""
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()

        main_content = soup.find('main') or soup.find('body') or soup
        text = main_content.get_text()

        lines = [line.strip() for line in text.split('\n') if line.strip()]
        return '\n'.join(lines) if lines else "í…ìŠ¤íŠ¸ ì—†ìŒ"

    def _save_incremental_data(self):
        """ì¦ë¶„ ë°ì´í„° ì €ì¥."""
        self._log_step_start("ë°ì´í„° ì €ì¥")
        
        # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
        merged_data = {}
        for data_type in ['main_categories', 'sub_categories', 'courses', 'course_details']:
            if data_type == 'courses':
                # ê°•ì˜ ë°ì´í„°ëŠ” URL ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
                existing_urls = set(item.get('ê°•ì˜ë§í¬', '') 
                                  for item in self.previous_data[data_type])
                new_items = [item for item in self.current_data[data_type]
                           if item.get('ê°•ì˜ë§í¬', '') not in existing_urls]
                merged_data[data_type] = self.previous_data[data_type] + new_items
            else:
                # ë‹¤ë¥¸ ë°ì´í„°ëŠ” í˜„ì¬ ë°ì´í„°ë¡œ êµì²´
                merged_data[data_type] = self.current_data[data_type]

        # JSON íŒŒì¼ ì €ì¥
        data_files = {
            'main_categories': 'main_categories.json',
            'sub_categories': 'sub_categories.json',
            'courses': 'courses_list.json',
            'course_details': 'course_details.json'
        }

        for data_type, filename in data_files.items():
            json_path = self.output_dir / "json_files" / filename
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(merged_data[data_type], f, ensure_ascii=False, indent=2)
            self._log_progress(f"{data_type} ì €ì¥ ì™„ë£Œ: {len(merged_data[data_type])}ê°œ")

        # ì¦ë¶„ ë¡œê·¸ ì €ì¥
        self._save_incremental_log()

        self._log_step_complete("ë°ì´í„° ì €ì¥")

    def _save_incremental_log(self):
        """ì¦ë¶„ í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥."""
        log_data = {
            "ì‹¤í–‰ì¼ì‹œ": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "ì²«ì‹¤í–‰ì—¬ë¶€": self.is_first_run,
            "ì¦ë¶„í†µê³„": self.incremental_stats,
            "ì²˜ë¦¬ì‹œê°„": time.time() - self.start_time
        }

        log_file = self.output_dir / "incremental_logs" / f"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)

    def _log_performance_improvement(self, elapsed_time):
        """ì„±ëŠ¥ ê°œì„  íš¨ê³¼ ë¡œê¹…."""
        estimated_full_time = 900  # 15ë¶„ ì˜ˆìƒ
        improvement_percent = ((estimated_full_time - elapsed_time) / estimated_full_time) * 100
        
        self._log_progress("=== ì„±ëŠ¥ ê°œì„  íš¨ê³¼ ===")
        self._log_progress(f"ì˜ˆìƒ ì „ì²´ í¬ë¡¤ë§ ì‹œê°„: {estimated_full_time}ì´ˆ")
        self._log_progress(f"ì‹¤ì œ ì¦ë¶„ í¬ë¡¤ë§ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        self._log_progress(f"ì‹œê°„ ë‹¨ì¶•: {improvement_percent:.1f}%")
        
        if improvement_percent > 90:
            self._log_progress("ğŸš€ ìš°ìˆ˜í•œ ì„±ëŠ¥ ê°œì„ !")
        elif improvement_percent > 70:
            self._log_progress("âœ… ì¢‹ì€ ì„±ëŠ¥ ê°œì„ !")
        else:
            self._log_progress("âš ï¸ ì„±ëŠ¥ ê°œì„  í•„ìš”")

    def run(self):
        """ì¦ë¶„ í¬ë¡¤ë§ ì‹¤í–‰."""
        self.start_time = time.time()
        self._log_progress("FastCampus ì¦ë¶„ í¬ë¡¤ë§ ì‹œì‘!")

        # ì´ì „ ë°ì´í„° ë¡œë“œ
        self._load_previous_data()

        if self.is_first_run:
            self._log_progress("ì²« ì‹¤í–‰ ê°ì§€ - ì „ì²´ í¬ë¡¤ë§ ëª¨ë“œ")
            # ì²« ì‹¤í–‰ ì‹œì—ëŠ” ê¸°ì¡´ í¬ë¡¤ëŸ¬ì™€ ë™ì¼í•˜ê²Œ ì‘ë™
            # (ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ë©”ì¸ ì¹´í…Œê³ ë¦¬ë§Œ í™•ì¸)
        else:
            self._log_progress("ì¬ì‹¤í–‰ ê°ì§€ - ì¦ë¶„ í¬ë¡¤ë§ ëª¨ë“œ")

        with sync_playwright() as p:
            browser, page = self._setup_browser(p)

            try:
                self._log_progress("ë¸Œë¼ìš°ì € ì„¤ì • ì™„ë£Œ")

                if not self._safe_page_load(page, self.base_url):
                    self._log_error("ë©”ì¸ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
                    return

                self._log_progress("ë©”ì¸ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")

                # ë©”ì¸ ì¹´í…Œê³ ë¦¬ í™•ì¸
                self._extract_main_categories(page)

                # í•˜ìœ„ ì¹´í…Œê³ ë¦¬ í™•ì¸
                self._extract_sub_categories(page)

                # ì‹ ê·œ/ì—…ë°ì´íŠ¸ëœ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ì—ì„œ ê°•ì˜ ìˆ˜ì§‘
                self._collect_courses_incremental()

                # ì‹ ê·œ/ì—…ë°ì´íŠ¸ëœ ê°•ì˜ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                self._collect_course_details_incremental()

                # ë°ì´í„° ì €ì¥
                self._save_incremental_data()

                # ì¦ë¶„ í¬ë¡¤ë§ í†µê³„ ì¶œë ¥
                self._log_incremental_stats()

                # ì™„ë£Œ
                elapsed_time = time.time() - self.start_time
                self._log_progress(f"ì¦ë¶„ í¬ë¡¤ë§ ì™„ë£Œ! ì´ ì†Œìš”ì‹œê°„: "
                                 f"{elapsed_time:.2f}ì´ˆ")
                
                # ì„±ëŠ¥ ê°œì„  íš¨ê³¼ í‘œì‹œ
                if not self.is_first_run:
                    self._log_performance_improvement(elapsed_time)

            except Exception as e:
                self._log_error("í¬ë¡¤ë§ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ", e)
            finally:
                browser.close()
                self._log_progress("ë¸Œë¼ìš°ì € ì¢…ë£Œ")


def main():
    """Application entry point."""
    crawler = IncrementalCrawler()
    crawler.run()


if __name__ == "__main__":
    main()
